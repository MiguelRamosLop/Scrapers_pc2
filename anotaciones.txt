Anotaciones: 

- Ejecutar los scrapers posibilita ingresar cualquier localidad/filtro. Recomendamos antes comprobar si en la misma
página el formato se conserva dado que puede variar y debe ser ingresado correctamente para que scrapee como es debido. 

- En cuanto a la mayoria de scrapers permiten hacer multiples requests sin limitaciones, sin embargo, Yelp da algunos 
problemas relacionados con esto. Una vez realizadas algunas requests a Yelp, este te impedirá seguir haciendo (y aunque
si les envias un correo te dan los permisos) creemos que es mejor usar otras estrategias que permitan seguir scrapeando
al momento. Para ello nos apoyamos en un Proxy, concretamente en windscribe (completamente gratuito) el cual es una 
extensión de google con la cual podemos acceder a Yelp con otra ip y poder ir haciendo las request sin limitaciones. Además,
deberemos también usar otra aplicacion como replit para ejecutar el código en la nube dado que desde el visual será imposible
una vez superado ese limite de requests. Conclusión: en caso de tener errores sin sentido o no scrapear correctamente, será
debido a ese limite de request por lo que deberemos usar windscribe y replit para seguir ejecutando el scraper pudiendo asi
ya ejecutarlo todo sin problemas.

- En todos los scrapers quedan comentadas diferentes "pruebas" que hemos ido comprobando durante la realización de cada uno.
Aunque estan comentadas, es posible y hasta recomendable, descomentar algunas para poder ir viendo el propio avance del 
scraper respecto a las diferentes funciones con las que cuenta. 

- Todos los scrapers siguen la misma estructura, indicada en el readme del proyecto en el GitHub.